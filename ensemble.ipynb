{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "JRX3MdSelM9E",
    "outputId": "f24d26c5-81de-4a78-e965-62d8ad4cdaef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jan  9 23:02:34 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 396.44                 Driver Version: 396.44                    |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           On   | 00003C97:00:00.0 Off |                    0 |\n",
      "| N/A   72C    P8    31W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# try:\n",
    "#     import os\n",
    "#     from google.colab.drive import mount\n",
    "#     mount('./drive')\n",
    "#     %cd 'drive/My Drive/Colab Notebooks/Final Project'\n",
    "#     if not os.path.exists('flower_data.zip'):\n",
    "# #     !wget https://s3.amazonaws.com/content.udacity-data.com/courses/nd188/flower_data.zip\n",
    "#         !wget https://s3.amazonaws.com/content.udacity-data.com/nd089/flower_data.tar.gz\n",
    "#     if not os.path.exists('flower_data'):\n",
    "# #     !unzip flower_data.zip\n",
    "#         !mkdir flower_data\n",
    "#         !tar -C ./flower_data -xvzf flower_data.tar.gz\n",
    "# except ImportError:\n",
    "#     pass\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "SodzctH8lVB0",
    "outputId": "c0d38575-5fb2-4af4-efe9-459362caf249"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.3.0\n"
     ]
    }
   ],
   "source": [
    "# !pip3 uninstall -y Pillow\n",
    "# !pip3 install Pillow==5.3.0\n",
    "# !pip3 install http://download.pytorch.org/whl/cu80/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n",
    "# # !pip3 install https://download.pytorch.org/whl/cu80/torch-0.4.0-cp36-cp36m-linux_x86_64.whl\n",
    "# !pip3 install torchvision\n",
    "import PIL\n",
    "print(PIL.PILLOW_VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0RtZDlyZpXrC"
   },
   "outputs": [],
   "source": [
    "# Imports here\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch import optim, nn\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qoTGvmQN1OB4"
   },
   "outputs": [],
   "source": [
    "# !pip install gdown==3.6.0\n",
    "# # https://drive.google.com/open?id=1W26FgrVjl-HHG77XL--v4s1mgE0a0ju1\n",
    "# my_file_id = '1W26FgrVjl-HHG77XL--v4s1mgE0a0ju1'\n",
    "# !gdown https://drive.google.com/uc?id={my_file_id}\n",
    "# !ls\n",
    "# !unzip big_data.zip\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "def allDone():\n",
    "    display(Audio(url='https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav', autoplay=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B3Zly2Oppayl"
   },
   "outputs": [],
   "source": [
    "data_dir = 'big_data'\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/valid'\n",
    "test_dir = data_dir + '/test'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gh8OvbJppd2t"
   },
   "outputs": [],
   "source": [
    "# TODO: Define your transforms for the training and validation sets\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "batch_size = 8\n",
    "data_transforms = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.RandomAffine(45, translate=(0.1,0.1), scale=(0.8,1.2), shear=10),\n",
    "        transforms.RandomHorizontalFlip(), transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "    \"valid\": transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "    \"test\": transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "}\n",
    "\n",
    "# TODO: Load the datasets with ImageFolder\n",
    "image_datasets = {\n",
    "    \"train\": ImageFolder(train_dir, transform=data_transforms[\"train\"]),\n",
    "    \"valid\": ImageFolder(valid_dir, transform=data_transforms[\"valid\"]),\n",
    "    \"test\": ImageFolder(valid_dir, transform=data_transforms[\"test\"])\n",
    "}\n",
    "\n",
    "# ratio = 0.25\n",
    "# num = int(ratio*len(image_datasets[\"train\"]))\n",
    "# scale_down,_ = random_split(image_datasets[\"train\"], [num, len(image_datasets[\"train\"])-num])\n",
    "# TODO: Using the image datasets and the trainforms, define the dataloaders\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": DataLoader(image_datasets[\"train\"], batch_size=batch_size, shuffle=True),\n",
    "    \"valid\": DataLoader(image_datasets[\"valid\"], batch_size=batch_size, shuffle=True),\n",
    "    \"test\": DataLoader(image_datasets[\"test\"], batch_size=batch_size, shuffle=True),\n",
    "}\n",
    "\n",
    "# dataloaders = {\n",
    "#     \"train\": DataLoader(scale_down, batch_size=batch_size, shuffle=True),\n",
    "#     \"valid\": DataLoader(image_datasets[\"valid\"], batch_size=batch_size, shuffle=True),\n",
    "#     \"test\": DataLoader(image_datasets[\"test\"], batch_size=batch_size, shuffle=True),\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1751
    },
    "colab_type": "code",
    "id": "ag9V2pN8pkeS",
    "outputId": "f63fa62c-0962-4844-bba5-ace5db0bacf1"
   },
   "outputs": [],
   "source": [
    "# with open('cat_to_name.json', 'r') as f:\n",
    "#     cat_to_name = json.load(f)\n",
    "# sorted(cat_to_name)\n",
    "cat_to_name = [0]*102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T8PtI7P-pngY"
   },
   "outputs": [],
   "source": [
    "def plt_stats(stats_error, stats_accuracy):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.title('stats')\n",
    "    plt.subplot(121)\n",
    "    plt.plot(stats_error['train'],label=\"train\")\n",
    "    plt.plot(stats_error['valid'], label=\"validation\")\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.title(\"LOSS\")\n",
    "    plt.subplot(122)\n",
    "    plt.plot(stats_accuracy['train'], label=\"train\")\n",
    "    plt.plot(stats_accuracy['valid'], label=\"validation\")\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend()\n",
    "    plt.title(\"ACCURACY\")\n",
    "    plt.savefig(str(time.time())+'.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3lRISpTypq_f"
   },
   "outputs": [],
   "source": [
    "def train(epochs, model, optimizer, scheduler, dataloaders, criterion, model_name=\"my_model.pt\", save=True, best_acc=0.0, stats_error = {'train':[], 'valid': []}, stats_accuracy = {'train':[], 'valid': []}):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'valid']:\n",
    "            print(phase, end='')\n",
    "            if phase == 'train':\n",
    "                if str(type(scheduler)) != \"<class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>\":\n",
    "                    scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                print('.', end='')\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data).item()\n",
    "            print()\n",
    "            \n",
    "            \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects / len(dataloaders[phase].dataset)\n",
    "            stats_error[phase].append(epoch_loss)\n",
    "            stats_accuracy[phase].append(epoch_acc)\n",
    "            \n",
    "            if phase==\"valid\" and str(type(scheduler)) == \"<class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>\":\n",
    "                scheduler.step(metrics = epoch_loss)\n",
    "            \n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "        # deep copy the model\n",
    "            if phase == 'valid' and epoch_acc >= best_acc:\n",
    "                print(\"accuracy increased from {:.4f} to {:.4f}, saving model....\".format(best_acc, epoch_acc))\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                if save:\n",
    "                    torch.save(model.state_dict(), model_name)\n",
    "        print()\n",
    "    \n",
    "    json.dump({\"stats_error\":stats_error, \"stats_accuracy\": stats_accuracy}, open('stats-'+str(time.time())+'.json', 'w'))\n",
    "    plt_stats(stats_error, stats_accuracy)\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    \n",
    "    allDone()\n",
    "    \n",
    "    return best_acc, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IFHo7Yr4qqR_"
   },
   "outputs": [],
   "source": [
    "def test(model, dataloaders):\n",
    "    model.eval()\n",
    "    running_corrects = 0.0\n",
    "    for batch, (inputs, labels) in enumerate(dataloaders['test']):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "        # statistics\n",
    "        accu =  torch.sum(preds == labels.data)\n",
    "        running_corrects += accu.data.item()\n",
    "        print(\"batch:\", batch+1, \"Accuracy:\", accu.data.item()/len(inputs))\n",
    "    print(\"overall accuracy:\", running_corrects/len(dataloaders['test'].dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NmscJ7PZr4Sc"
   },
   "outputs": [],
   "source": [
    "def google_test(model):\n",
    "    !git clone https://github.com/GabrielePicco/deep-learning-flower-identifier\n",
    "    !pip install airtable\n",
    "    import sys\n",
    "    sys.path.insert(0, 'deep-learning-flower-identifier')\n",
    "    \n",
    "    import PIL\n",
    "    print(PIL.PILLOW_VERSION)\n",
    "\n",
    "    # model = model.to(device)\n",
    "    model.eval()\n",
    "    from test_model_pytorch_facebook_challenge import calc_accuracy\n",
    "    calc_accuracy(model, input_image_size=224, use_google_testset=True, batch_size=batch_size, norm_mean = mean, norm_std=std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.model1 = torchvision.models.resnet152(pretrained=False)\n",
    "        in_features = self.model1.fc.in_features\n",
    "        out_features = 102\n",
    "        fc = nn.Sequential(\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, out_features)\n",
    "        )\n",
    "        self.model1.fc = fc\n",
    "        self.model2 = torchvision.models.resnet152(pretrained=False)\n",
    "        fc = nn.Linear(in_features, out_features)\n",
    "        self.model2.fc = fc\n",
    "        self.relu = nn.ReLU(0.2)\n",
    "#         self.linear = nn.Linear(102*2,64)\n",
    "#         self.linear1 = nn.Linear(64,102)\n",
    "        self.model1.load_state_dict(torch.load('goog1.pt'))\n",
    "        self.model2.load_state_dict(torch.load('goog.pt'))\n",
    "        self.model1.requires_grad = False\n",
    "        self.model2.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        m1 = self.model1(x)\n",
    "        m2 = self.model2(x)\n",
    "#         m1,_ = torch.max(self.model1(x),1)\n",
    "#         m2,_ = torch.max(self.model2(x),1)\n",
    "#         x = torch.cat((m1, m2), 1)\n",
    "#         return self.linear1(nn.Dropout(0.2)(self.linear(x)))\n",
    "        return (m1+m2)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "TcRpc52TsY2p",
    "outputId": "4da32e84-c78d-4cda-df15-fc962a9bc4f5"
   },
   "outputs": [],
   "source": [
    "# TODO: Build and train your network\n",
    "model = Model()\n",
    "\n",
    "length = len(list(model.parameters()))\n",
    "\n",
    "model.model1.requires_grad = False\n",
    "model.model2.requires_grad = False\n",
    "# in_features = model.fc.in_features\n",
    "# in_features = model.classifier[0].in_features\n",
    "# out_features = len(cat_to_name)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wvHb9_h5xNSJ",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "lr = 1e-1\n",
    "gamma = 0.1\n",
    "step_size = 5\n",
    "momentum = 0.8\n",
    "cooldown = 1\n",
    "best_acc = 0.0\n",
    "# model.load_state_dict(torch.load('ensemble.pt'))\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum = momentum)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, patience=step_size, cooldown=cooldown,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "colab_type": "code",
    "id": "KlcHtQUAy8DR",
    "outputId": "f1ab45fb-3743-4bbc-ee4b-96debf3e53b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "----------\n",
      "train...............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "train Loss: nan Acc: 0.0053\n",
      "valid..............................................................................................................................................................................................................................................."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-89020dba994f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ensemble.pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-b4432cf6100d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, model, optimizer, scheduler, dataloaders, criterion, model_name, save, best_acc, stats_error, stats_accuracy)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;31m# track history if only in train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/py35/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-af37c28e2670>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mm1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mm2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/py35/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/py35/lib/python3.5/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/py35/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/py35/lib/python3.5/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/py35/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/py35/lib/python3.5/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/py35/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/py35/lib/python3.5/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/py35/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1621\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1622\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1623\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1624\u001b[0m     )\n\u001b[1;32m   1625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_acc, model = train(epochs, model, optimizer, scheduler, dataloaders, criterion, model_name=\"ensemble.pt\", best_acc = best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1 Accuracy: 1.0\n",
      "batch: 2 Accuracy: 1.0\n",
      "batch: 3 Accuracy: 1.0\n",
      "batch: 4 Accuracy: 1.0\n",
      "batch: 5 Accuracy: 1.0\n",
      "batch: 6 Accuracy: 1.0\n",
      "batch: 7 Accuracy: 1.0\n",
      "batch: 8 Accuracy: 1.0\n",
      "batch: 9 Accuracy: 1.0\n",
      "batch: 10 Accuracy: 1.0\n",
      "batch: 11 Accuracy: 1.0\n",
      "batch: 12 Accuracy: 1.0\n",
      "batch: 13 Accuracy: 1.0\n",
      "batch: 14 Accuracy: 1.0\n",
      "batch: 15 Accuracy: 1.0\n",
      "batch: 16 Accuracy: 1.0\n",
      "batch: 17 Accuracy: 1.0\n",
      "batch: 18 Accuracy: 1.0\n",
      "batch: 19 Accuracy: 1.0\n",
      "batch: 20 Accuracy: 1.0\n",
      "batch: 21 Accuracy: 1.0\n",
      "batch: 22 Accuracy: 1.0\n",
      "batch: 23 Accuracy: 1.0\n",
      "batch: 24 Accuracy: 1.0\n",
      "batch: 25 Accuracy: 1.0\n",
      "batch: 26 Accuracy: 1.0\n",
      "batch: 27 Accuracy: 1.0\n",
      "batch: 28 Accuracy: 1.0\n",
      "batch: 29 Accuracy: 1.0\n",
      "batch: 30 Accuracy: 1.0\n",
      "batch: 31 Accuracy: 1.0\n",
      "batch: 32 Accuracy: 1.0\n",
      "batch: 33 Accuracy: 1.0\n",
      "batch: 34 Accuracy: 1.0\n",
      "batch: 35 Accuracy: 1.0\n",
      "batch: 36 Accuracy: 1.0\n",
      "batch: 37 Accuracy: 1.0\n",
      "batch: 38 Accuracy: 1.0\n",
      "batch: 39 Accuracy: 1.0\n",
      "batch: 40 Accuracy: 1.0\n",
      "batch: 41 Accuracy: 1.0\n",
      "batch: 42 Accuracy: 1.0\n",
      "batch: 43 Accuracy: 1.0\n",
      "batch: 44 Accuracy: 1.0\n",
      "batch: 45 Accuracy: 1.0\n",
      "batch: 46 Accuracy: 1.0\n",
      "batch: 47 Accuracy: 1.0\n",
      "batch: 48 Accuracy: 1.0\n",
      "batch: 49 Accuracy: 1.0\n",
      "batch: 50 Accuracy: 1.0\n",
      "batch: 51 Accuracy: 1.0\n",
      "batch: 52 Accuracy: 1.0\n",
      "batch: 53 Accuracy: 1.0\n",
      "batch: 54 Accuracy: 1.0\n",
      "batch: 55 Accuracy: 1.0\n",
      "batch: 56 Accuracy: 1.0\n",
      "batch: 57 Accuracy: 1.0\n",
      "batch: 58 Accuracy: 1.0\n",
      "batch: 59 Accuracy: 1.0\n",
      "batch: 60 Accuracy: 1.0\n",
      "batch: 61 Accuracy: 1.0\n",
      "batch: 62 Accuracy: 1.0\n",
      "batch: 63 Accuracy: 1.0\n",
      "batch: 64 Accuracy: 1.0\n",
      "batch: 65 Accuracy: 1.0\n",
      "batch: 66 Accuracy: 1.0\n",
      "batch: 67 Accuracy: 1.0\n",
      "batch: 68 Accuracy: 1.0\n",
      "batch: 69 Accuracy: 1.0\n",
      "batch: 70 Accuracy: 1.0\n",
      "batch: 71 Accuracy: 1.0\n",
      "batch: 72 Accuracy: 1.0\n",
      "batch: 73 Accuracy: 1.0\n",
      "batch: 74 Accuracy: 1.0\n",
      "batch: 75 Accuracy: 1.0\n",
      "batch: 76 Accuracy: 1.0\n",
      "batch: 77 Accuracy: 1.0\n",
      "batch: 78 Accuracy: 1.0\n",
      "batch: 79 Accuracy: 1.0\n",
      "batch: 80 Accuracy: 1.0\n",
      "batch: 81 Accuracy: 1.0\n",
      "batch: 82 Accuracy: 1.0\n",
      "batch: 83 Accuracy: 1.0\n",
      "batch: 84 Accuracy: 1.0\n",
      "batch: 85 Accuracy: 1.0\n",
      "batch: 86 Accuracy: 1.0\n",
      "batch: 87 Accuracy: 1.0\n",
      "batch: 88 Accuracy: 1.0\n",
      "batch: 89 Accuracy: 1.0\n",
      "batch: 90 Accuracy: 1.0\n",
      "batch: 91 Accuracy: 1.0\n",
      "batch: 92 Accuracy: 1.0\n",
      "batch: 93 Accuracy: 1.0\n",
      "batch: 94 Accuracy: 1.0\n",
      "batch: 95 Accuracy: 1.0\n",
      "batch: 96 Accuracy: 1.0\n",
      "batch: 97 Accuracy: 1.0\n",
      "batch: 98 Accuracy: 1.0\n",
      "batch: 99 Accuracy: 1.0\n",
      "batch: 100 Accuracy: 1.0\n",
      "batch: 101 Accuracy: 1.0\n",
      "batch: 102 Accuracy: 1.0\n",
      "batch: 103 Accuracy: 1.0\n",
      "batch: 104 Accuracy: 1.0\n",
      "batch: 105 Accuracy: 1.0\n",
      "batch: 106 Accuracy: 1.0\n",
      "batch: 107 Accuracy: 1.0\n",
      "batch: 108 Accuracy: 1.0\n",
      "batch: 109 Accuracy: 1.0\n",
      "batch: 110 Accuracy: 1.0\n",
      "batch: 111 Accuracy: 1.0\n",
      "batch: 112 Accuracy: 1.0\n",
      "batch: 113 Accuracy: 1.0\n",
      "batch: 114 Accuracy: 1.0\n",
      "batch: 115 Accuracy: 1.0\n",
      "batch: 116 Accuracy: 1.0\n",
      "batch: 117 Accuracy: 1.0\n",
      "batch: 118 Accuracy: 1.0\n",
      "batch: 119 Accuracy: 1.0\n",
      "batch: 120 Accuracy: 1.0\n",
      "batch: 121 Accuracy: 1.0\n",
      "batch: 122 Accuracy: 1.0\n",
      "batch: 123 Accuracy: 1.0\n",
      "batch: 124 Accuracy: 1.0\n",
      "batch: 125 Accuracy: 1.0\n",
      "batch: 126 Accuracy: 1.0\n",
      "batch: 127 Accuracy: 1.0\n",
      "batch: 128 Accuracy: 1.0\n",
      "batch: 129 Accuracy: 1.0\n",
      "batch: 130 Accuracy: 1.0\n",
      "batch: 131 Accuracy: 1.0\n",
      "batch: 132 Accuracy: 1.0\n",
      "batch: 133 Accuracy: 0.875\n",
      "batch: 134 Accuracy: 1.0\n",
      "batch: 135 Accuracy: 1.0\n",
      "batch: 136 Accuracy: 1.0\n",
      "batch: 137 Accuracy: 1.0\n",
      "batch: 138 Accuracy: 1.0\n",
      "batch: 139 Accuracy: 1.0\n",
      "batch: 140 Accuracy: 1.0\n",
      "batch: 141 Accuracy: 1.0\n",
      "batch: 142 Accuracy: 1.0\n",
      "batch: 143 Accuracy: 1.0\n",
      "batch: 144 Accuracy: 1.0\n",
      "batch: 145 Accuracy: 1.0\n",
      "batch: 146 Accuracy: 1.0\n",
      "batch: 147 Accuracy: 1.0\n",
      "batch: 148 Accuracy: 1.0\n",
      "batch: 149 Accuracy: 1.0\n",
      "batch: 150 Accuracy: 1.0\n",
      "batch: 151 Accuracy: 1.0\n",
      "batch: 152 Accuracy: 1.0\n",
      "batch: 153 Accuracy: 1.0\n",
      "batch: 154 Accuracy: 1.0\n",
      "batch: 155 Accuracy: 1.0\n",
      "batch: 156 Accuracy: 1.0\n",
      "batch: 157 Accuracy: 1.0\n",
      "batch: 158 Accuracy: 1.0\n",
      "batch: 159 Accuracy: 1.0\n",
      "batch: 160 Accuracy: 1.0\n",
      "batch: 161 Accuracy: 1.0\n",
      "batch: 162 Accuracy: 1.0\n",
      "batch: 163 Accuracy: 1.0\n",
      "batch: 164 Accuracy: 1.0\n",
      "batch: 165 Accuracy: 1.0\n",
      "batch: 166 Accuracy: 1.0\n",
      "batch: 167 Accuracy: 1.0\n",
      "batch: 168 Accuracy: 1.0\n",
      "batch: 169 Accuracy: 1.0\n",
      "batch: 170 Accuracy: 1.0\n",
      "batch: 171 Accuracy: 1.0\n",
      "batch: 172 Accuracy: 1.0\n",
      "batch: 173 Accuracy: 1.0\n",
      "batch: 174 Accuracy: 1.0\n",
      "batch: 175 Accuracy: 1.0\n",
      "batch: 176 Accuracy: 1.0\n",
      "batch: 177 Accuracy: 1.0\n",
      "batch: 178 Accuracy: 1.0\n",
      "batch: 179 Accuracy: 1.0\n",
      "batch: 180 Accuracy: 1.0\n",
      "batch: 181 Accuracy: 1.0\n",
      "batch: 182 Accuracy: 1.0\n",
      "batch: 183 Accuracy: 1.0\n",
      "batch: 184 Accuracy: 1.0\n",
      "batch: 185 Accuracy: 1.0\n",
      "batch: 186 Accuracy: 1.0\n",
      "batch: 187 Accuracy: 1.0\n",
      "batch: 188 Accuracy: 1.0\n",
      "batch: 189 Accuracy: 1.0\n",
      "batch: 190 Accuracy: 1.0\n",
      "batch: 191 Accuracy: 1.0\n",
      "batch: 192 Accuracy: 1.0\n",
      "batch: 193 Accuracy: 1.0\n",
      "batch: 194 Accuracy: 1.0\n",
      "batch: 195 Accuracy: 1.0\n",
      "batch: 196 Accuracy: 1.0\n",
      "batch: 197 Accuracy: 1.0\n",
      "batch: 198 Accuracy: 1.0\n",
      "batch: 199 Accuracy: 1.0\n",
      "batch: 200 Accuracy: 1.0\n",
      "batch: 201 Accuracy: 1.0\n",
      "batch: 202 Accuracy: 1.0\n",
      "batch: 203 Accuracy: 1.0\n",
      "batch: 204 Accuracy: 1.0\n",
      "batch: 205 Accuracy: 1.0\n",
      "batch: 206 Accuracy: 1.0\n",
      "batch: 207 Accuracy: 1.0\n",
      "batch: 208 Accuracy: 1.0\n",
      "batch: 209 Accuracy: 1.0\n",
      "batch: 210 Accuracy: 1.0\n",
      "batch: 211 Accuracy: 1.0\n",
      "batch: 212 Accuracy: 1.0\n",
      "batch: 213 Accuracy: 1.0\n",
      "batch: 214 Accuracy: 1.0\n",
      "batch: 215 Accuracy: 1.0\n",
      "batch: 216 Accuracy: 1.0\n",
      "batch: 217 Accuracy: 1.0\n",
      "batch: 218 Accuracy: 1.0\n",
      "batch: 219 Accuracy: 1.0\n",
      "batch: 220 Accuracy: 1.0\n",
      "batch: 221 Accuracy: 1.0\n",
      "batch: 222 Accuracy: 1.0\n",
      "batch: 223 Accuracy: 1.0\n",
      "batch: 224 Accuracy: 1.0\n",
      "batch: 225 Accuracy: 1.0\n",
      "batch: 226 Accuracy: 1.0\n",
      "batch: 227 Accuracy: 1.0\n",
      "batch: 228 Accuracy: 1.0\n",
      "batch: 229 Accuracy: 1.0\n",
      "batch: 230 Accuracy: 1.0\n",
      "batch: 231 Accuracy: 1.0\n",
      "batch: 232 Accuracy: 1.0\n",
      "batch: 233 Accuracy: 1.0\n",
      "batch: 234 Accuracy: 1.0\n",
      "batch: 235 Accuracy: 1.0\n",
      "batch: 236 Accuracy: 1.0\n",
      "batch: 237 Accuracy: 1.0\n",
      "batch: 238 Accuracy: 1.0\n",
      "batch: 239 Accuracy: 1.0\n",
      "batch: 240 Accuracy: 1.0\n",
      "batch: 241 Accuracy: 1.0\n",
      "batch: 242 Accuracy: 1.0\n",
      "batch: 243 Accuracy: 1.0\n",
      "batch: 244 Accuracy: 1.0\n",
      "batch: 245 Accuracy: 1.0\n",
      "batch: 246 Accuracy: 1.0\n",
      "batch: 247 Accuracy: 1.0\n",
      "batch: 248 Accuracy: 1.0\n",
      "batch: 249 Accuracy: 1.0\n",
      "batch: 250 Accuracy: 1.0\n",
      "batch: 251 Accuracy: 1.0\n",
      "batch: 252 Accuracy: 1.0\n",
      "batch: 253 Accuracy: 1.0\n",
      "batch: 254 Accuracy: 1.0\n",
      "batch: 255 Accuracy: 1.0\n",
      "batch: 256 Accuracy: 1.0\n",
      "batch: 257 Accuracy: 1.0\n",
      "batch: 258 Accuracy: 1.0\n",
      "batch: 259 Accuracy: 1.0\n",
      "batch: 260 Accuracy: 1.0\n",
      "batch: 261 Accuracy: 1.0\n",
      "batch: 262 Accuracy: 1.0\n",
      "batch: 263 Accuracy: 1.0\n",
      "batch: 264 Accuracy: 1.0\n",
      "batch: 265 Accuracy: 1.0\n",
      "batch: 266 Accuracy: 1.0\n",
      "batch: 267 Accuracy: 1.0\n",
      "batch: 268 Accuracy: 1.0\n",
      "batch: 269 Accuracy: 1.0\n",
      "batch: 270 Accuracy: 1.0\n",
      "batch: 271 Accuracy: 1.0\n",
      "batch: 272 Accuracy: 1.0\n",
      "batch: 273 Accuracy: 1.0\n",
      "batch: 274 Accuracy: 1.0\n",
      "batch: 275 Accuracy: 1.0\n",
      "batch: 276 Accuracy: 1.0\n",
      "batch: 277 Accuracy: 1.0\n",
      "batch: 278 Accuracy: 1.0\n",
      "batch: 279 Accuracy: 1.0\n",
      "batch: 280 Accuracy: 1.0\n",
      "batch: 281 Accuracy: 1.0\n",
      "batch: 282 Accuracy: 1.0\n",
      "batch: 283 Accuracy: 1.0\n",
      "batch: 284 Accuracy: 1.0\n",
      "batch: 285 Accuracy: 1.0\n",
      "batch: 286 Accuracy: 1.0\n",
      "batch: 287 Accuracy: 1.0\n",
      "batch: 288 Accuracy: 1.0\n",
      "batch: 289 Accuracy: 1.0\n",
      "batch: 290 Accuracy: 1.0\n",
      "batch: 291 Accuracy: 1.0\n",
      "batch: 292 Accuracy: 1.0\n",
      "batch: 293 Accuracy: 1.0\n",
      "batch: 294 Accuracy: 1.0\n",
      "batch: 295 Accuracy: 1.0\n",
      "batch: 296 Accuracy: 1.0\n",
      "batch: 297 Accuracy: 1.0\n",
      "batch: 298 Accuracy: 1.0\n",
      "batch: 299 Accuracy: 1.0\n",
      "batch: 300 Accuracy: 1.0\n",
      "batch: 301 Accuracy: 1.0\n",
      "batch: 302 Accuracy: 1.0\n",
      "batch: 303 Accuracy: 1.0\n",
      "batch: 304 Accuracy: 1.0\n",
      "batch: 305 Accuracy: 1.0\n",
      "batch: 306 Accuracy: 1.0\n",
      "batch: 307 Accuracy: 1.0\n",
      "batch: 308 Accuracy: 1.0\n",
      "batch: 309 Accuracy: 1.0\n",
      "batch: 310 Accuracy: 1.0\n",
      "batch: 311 Accuracy: 1.0\n",
      "batch: 312 Accuracy: 1.0\n",
      "batch: 313 Accuracy: 1.0\n",
      "batch: 314 Accuracy: 1.0\n",
      "batch: 315 Accuracy: 1.0\n",
      "batch: 316 Accuracy: 1.0\n",
      "batch: 317 Accuracy: 1.0\n",
      "batch: 318 Accuracy: 1.0\n",
      "batch: 319 Accuracy: 1.0\n",
      "batch: 320 Accuracy: 1.0\n",
      "batch: 321 Accuracy: 1.0\n",
      "batch: 322 Accuracy: 1.0\n",
      "batch: 323 Accuracy: 1.0\n",
      "batch: 324 Accuracy: 1.0\n",
      "batch: 325 Accuracy: 1.0\n",
      "batch: 326 Accuracy: 1.0\n",
      "batch: 327 Accuracy: 1.0\n",
      "batch: 328 Accuracy: 1.0\n",
      "batch: 329 Accuracy: 1.0\n",
      "batch: 330 Accuracy: 1.0\n",
      "batch: 331 Accuracy: 1.0\n",
      "batch: 332 Accuracy: 1.0\n",
      "batch: 333 Accuracy: 1.0\n",
      "batch: 334 Accuracy: 1.0\n",
      "batch: 335 Accuracy: 1.0\n",
      "batch: 336 Accuracy: 1.0\n",
      "batch: 337 Accuracy: 1.0\n",
      "batch: 338 Accuracy: 1.0\n",
      "batch: 339 Accuracy: 1.0\n",
      "batch: 340 Accuracy: 1.0\n",
      "batch: 341 Accuracy: 1.0\n",
      "batch: 342 Accuracy: 1.0\n",
      "batch: 343 Accuracy: 1.0\n",
      "batch: 344 Accuracy: 1.0\n",
      "batch: 345 Accuracy: 1.0\n",
      "batch: 346 Accuracy: 1.0\n",
      "batch: 347 Accuracy: 1.0\n",
      "batch: 348 Accuracy: 1.0\n",
      "batch: 349 Accuracy: 1.0\n",
      "batch: 350 Accuracy: 1.0\n",
      "batch: 351 Accuracy: 1.0\n",
      "batch: 352 Accuracy: 1.0\n",
      "batch: 353 Accuracy: 1.0\n",
      "batch: 354 Accuracy: 1.0\n",
      "batch: 355 Accuracy: 1.0\n",
      "batch: 356 Accuracy: 1.0\n",
      "batch: 357 Accuracy: 1.0\n",
      "batch: 358 Accuracy: 1.0\n",
      "batch: 359 Accuracy: 1.0\n",
      "batch: 360 Accuracy: 1.0\n",
      "batch: 361 Accuracy: 1.0\n",
      "batch: 362 Accuracy: 1.0\n",
      "batch: 363 Accuracy: 1.0\n",
      "batch: 364 Accuracy: 1.0\n",
      "batch: 365 Accuracy: 1.0\n",
      "batch: 366 Accuracy: 1.0\n",
      "batch: 367 Accuracy: 1.0\n",
      "batch: 368 Accuracy: 1.0\n",
      "batch: 369 Accuracy: 1.0\n",
      "batch: 370 Accuracy: 1.0\n",
      "batch: 371 Accuracy: 1.0\n",
      "batch: 372 Accuracy: 1.0\n",
      "batch: 373 Accuracy: 1.0\n",
      "batch: 374 Accuracy: 1.0\n",
      "batch: 375 Accuracy: 1.0\n",
      "batch: 376 Accuracy: 1.0\n",
      "batch: 377 Accuracy: 1.0\n",
      "batch: 378 Accuracy: 1.0\n",
      "batch: 379 Accuracy: 1.0\n",
      "batch: 380 Accuracy: 1.0\n",
      "batch: 381 Accuracy: 1.0\n",
      "batch: 382 Accuracy: 1.0\n",
      "batch: 383 Accuracy: 1.0\n",
      "batch: 384 Accuracy: 1.0\n",
      "batch: 385 Accuracy: 1.0\n",
      "batch: 386 Accuracy: 1.0\n",
      "batch: 387 Accuracy: 1.0\n",
      "batch: 388 Accuracy: 1.0\n",
      "batch: 389 Accuracy: 1.0\n",
      "batch: 390 Accuracy: 1.0\n",
      "batch: 391 Accuracy: 1.0\n",
      "batch: 392 Accuracy: 1.0\n",
      "batch: 393 Accuracy: 1.0\n",
      "batch: 394 Accuracy: 1.0\n",
      "batch: 395 Accuracy: 1.0\n",
      "batch: 396 Accuracy: 1.0\n",
      "batch: 397 Accuracy: 1.0\n",
      "batch: 398 Accuracy: 1.0\n",
      "batch: 399 Accuracy: 1.0\n",
      "batch: 400 Accuracy: 1.0\n",
      "batch: 401 Accuracy: 1.0\n",
      "batch: 402 Accuracy: 1.0\n",
      "batch: 403 Accuracy: 1.0\n",
      "batch: 404 Accuracy: 1.0\n",
      "batch: 405 Accuracy: 1.0\n",
      "batch: 406 Accuracy: 1.0\n",
      "batch: 407 Accuracy: 1.0\n",
      "batch: 408 Accuracy: 1.0\n",
      "batch: 409 Accuracy: 1.0\n",
      "batch: 410 Accuracy: 1.0\n",
      "batch: 411 Accuracy: 1.0\n",
      "batch: 412 Accuracy: 1.0\n",
      "batch: 413 Accuracy: 1.0\n",
      "batch: 414 Accuracy: 1.0\n",
      "batch: 415 Accuracy: 1.0\n",
      "batch: 416 Accuracy: 1.0\n",
      "batch: 417 Accuracy: 1.0\n",
      "batch: 418 Accuracy: 1.0\n",
      "batch: 419 Accuracy: 1.0\n",
      "batch: 420 Accuracy: 1.0\n",
      "batch: 421 Accuracy: 1.0\n",
      "batch: 422 Accuracy: 1.0\n",
      "batch: 423 Accuracy: 1.0\n",
      "batch: 424 Accuracy: 1.0\n",
      "batch: 425 Accuracy: 1.0\n",
      "batch: 426 Accuracy: 1.0\n",
      "batch: 427 Accuracy: 1.0\n",
      "batch: 428 Accuracy: 1.0\n",
      "batch: 429 Accuracy: 1.0\n",
      "batch: 430 Accuracy: 1.0\n",
      "batch: 431 Accuracy: 1.0\n",
      "batch: 432 Accuracy: 1.0\n",
      "batch: 433 Accuracy: 1.0\n",
      "batch: 434 Accuracy: 1.0\n",
      "batch: 435 Accuracy: 1.0\n",
      "batch: 436 Accuracy: 1.0\n",
      "batch: 437 Accuracy: 1.0\n",
      "batch: 438 Accuracy: 1.0\n",
      "batch: 439 Accuracy: 1.0\n",
      "batch: 440 Accuracy: 1.0\n",
      "batch: 441 Accuracy: 1.0\n",
      "batch: 442 Accuracy: 1.0\n",
      "batch: 443 Accuracy: 1.0\n",
      "batch: 444 Accuracy: 1.0\n",
      "batch: 445 Accuracy: 1.0\n",
      "batch: 446 Accuracy: 1.0\n",
      "batch: 447 Accuracy: 1.0\n",
      "batch: 448 Accuracy: 1.0\n",
      "batch: 449 Accuracy: 1.0\n",
      "batch: 450 Accuracy: 1.0\n",
      "batch: 451 Accuracy: 1.0\n",
      "batch: 452 Accuracy: 1.0\n",
      "batch: 453 Accuracy: 1.0\n",
      "batch: 454 Accuracy: 1.0\n",
      "batch: 455 Accuracy: 1.0\n",
      "batch: 456 Accuracy: 1.0\n",
      "batch: 457 Accuracy: 1.0\n",
      "batch: 458 Accuracy: 1.0\n",
      "batch: 459 Accuracy: 1.0\n",
      "batch: 460 Accuracy: 1.0\n",
      "batch: 461 Accuracy: 1.0\n",
      "batch: 462 Accuracy: 1.0\n",
      "batch: 463 Accuracy: 1.0\n",
      "batch: 464 Accuracy: 1.0\n",
      "batch: 465 Accuracy: 1.0\n",
      "batch: 466 Accuracy: 1.0\n",
      "batch: 467 Accuracy: 1.0\n",
      "batch: 468 Accuracy: 1.0\n",
      "batch: 469 Accuracy: 1.0\n",
      "batch: 470 Accuracy: 1.0\n",
      "batch: 471 Accuracy: 1.0\n",
      "batch: 472 Accuracy: 1.0\n",
      "batch: 473 Accuracy: 1.0\n",
      "batch: 474 Accuracy: 1.0\n",
      "batch: 475 Accuracy: 1.0\n",
      "batch: 476 Accuracy: 1.0\n",
      "batch: 477 Accuracy: 1.0\n",
      "batch: 478 Accuracy: 1.0\n",
      "batch: 479 Accuracy: 1.0\n",
      "batch: 480 Accuracy: 1.0\n",
      "batch: 481 Accuracy: 1.0\n",
      "batch: 482 Accuracy: 1.0\n",
      "batch: 483 Accuracy: 1.0\n",
      "batch: 484 Accuracy: 1.0\n",
      "batch: 485 Accuracy: 1.0\n",
      "batch: 486 Accuracy: 1.0\n",
      "batch: 487 Accuracy: 1.0\n",
      "batch: 488 Accuracy: 1.0\n",
      "batch: 489 Accuracy: 1.0\n",
      "batch: 490 Accuracy: 1.0\n",
      "batch: 491 Accuracy: 1.0\n",
      "batch: 492 Accuracy: 1.0\n",
      "batch: 493 Accuracy: 1.0\n",
      "batch: 494 Accuracy: 1.0\n",
      "batch: 495 Accuracy: 1.0\n",
      "batch: 496 Accuracy: 1.0\n",
      "batch: 497 Accuracy: 1.0\n",
      "batch: 498 Accuracy: 1.0\n",
      "batch: 499 Accuracy: 1.0\n",
      "batch: 500 Accuracy: 1.0\n",
      "batch: 501 Accuracy: 1.0\n",
      "batch: 502 Accuracy: 1.0\n",
      "batch: 503 Accuracy: 1.0\n",
      "batch: 504 Accuracy: 1.0\n",
      "batch: 505 Accuracy: 1.0\n",
      "batch: 506 Accuracy: 1.0\n",
      "batch: 507 Accuracy: 1.0\n",
      "batch: 508 Accuracy: 1.0\n",
      "batch: 509 Accuracy: 1.0\n",
      "batch: 510 Accuracy: 1.0\n",
      "batch: 511 Accuracy: 1.0\n",
      "batch: 512 Accuracy: 1.0\n",
      "batch: 513 Accuracy: 1.0\n",
      "batch: 514 Accuracy: 1.0\n",
      "batch: 515 Accuracy: 1.0\n",
      "batch: 516 Accuracy: 1.0\n",
      "batch: 517 Accuracy: 1.0\n",
      "batch: 518 Accuracy: 1.0\n",
      "batch: 519 Accuracy: 1.0\n",
      "batch: 520 Accuracy: 1.0\n",
      "batch: 521 Accuracy: 1.0\n",
      "batch: 522 Accuracy: 1.0\n",
      "batch: 523 Accuracy: 1.0\n",
      "batch: 524 Accuracy: 1.0\n",
      "batch: 525 Accuracy: 1.0\n",
      "batch: 526 Accuracy: 1.0\n",
      "batch: 527 Accuracy: 1.0\n",
      "batch: 528 Accuracy: 1.0\n",
      "batch: 529 Accuracy: 1.0\n",
      "batch: 530 Accuracy: 1.0\n",
      "batch: 531 Accuracy: 1.0\n",
      "batch: 532 Accuracy: 1.0\n",
      "batch: 533 Accuracy: 1.0\n",
      "batch: 534 Accuracy: 1.0\n",
      "batch: 535 Accuracy: 1.0\n",
      "batch: 536 Accuracy: 1.0\n",
      "batch: 537 Accuracy: 1.0\n",
      "batch: 538 Accuracy: 1.0\n",
      "batch: 539 Accuracy: 1.0\n",
      "batch: 540 Accuracy: 1.0\n",
      "batch: 541 Accuracy: 1.0\n",
      "batch: 542 Accuracy: 1.0\n",
      "batch: 543 Accuracy: 1.0\n",
      "batch: 544 Accuracy: 1.0\n",
      "batch: 545 Accuracy: 1.0\n",
      "batch: 546 Accuracy: 1.0\n",
      "batch: 547 Accuracy: 1.0\n",
      "batch: 548 Accuracy: 1.0\n",
      "batch: 549 Accuracy: 1.0\n",
      "batch: 550 Accuracy: 1.0\n",
      "batch: 551 Accuracy: 1.0\n",
      "batch: 552 Accuracy: 1.0\n",
      "batch: 553 Accuracy: 1.0\n",
      "batch: 554 Accuracy: 1.0\n",
      "batch: 555 Accuracy: 1.0\n",
      "batch: 556 Accuracy: 1.0\n",
      "batch: 557 Accuracy: 1.0\n",
      "batch: 558 Accuracy: 1.0\n",
      "batch: 559 Accuracy: 1.0\n",
      "batch: 560 Accuracy: 1.0\n",
      "batch: 561 Accuracy: 1.0\n",
      "batch: 562 Accuracy: 1.0\n",
      "batch: 563 Accuracy: 0.875\n",
      "batch: 564 Accuracy: 1.0\n",
      "batch: 565 Accuracy: 1.0\n",
      "batch: 566 Accuracy: 1.0\n",
      "batch: 567 Accuracy: 1.0\n",
      "batch: 568 Accuracy: 1.0\n",
      "batch: 569 Accuracy: 1.0\n",
      "batch: 570 Accuracy: 1.0\n",
      "batch: 571 Accuracy: 1.0\n",
      "batch: 572 Accuracy: 1.0\n",
      "batch: 573 Accuracy: 1.0\n",
      "batch: 574 Accuracy: 1.0\n",
      "batch: 575 Accuracy: 1.0\n",
      "batch: 576 Accuracy: 1.0\n",
      "batch: 577 Accuracy: 1.0\n",
      "batch: 578 Accuracy: 0.875\n",
      "batch: 579 Accuracy: 1.0\n",
      "batch: 580 Accuracy: 1.0\n",
      "batch: 581 Accuracy: 1.0\n",
      "batch: 582 Accuracy: 1.0\n",
      "batch: 583 Accuracy: 0.875\n",
      "batch: 584 Accuracy: 1.0\n",
      "batch: 585 Accuracy: 1.0\n",
      "batch: 586 Accuracy: 1.0\n",
      "batch: 587 Accuracy: 1.0\n",
      "batch: 588 Accuracy: 1.0\n",
      "batch: 589 Accuracy: 1.0\n",
      "batch: 590 Accuracy: 1.0\n",
      "batch: 591 Accuracy: 1.0\n",
      "batch: 592 Accuracy: 1.0\n",
      "batch: 593 Accuracy: 1.0\n",
      "batch: 594 Accuracy: 1.0\n",
      "batch: 595 Accuracy: 1.0\n",
      "batch: 596 Accuracy: 1.0\n",
      "batch: 597 Accuracy: 1.0\n",
      "batch: 598 Accuracy: 1.0\n",
      "batch: 599 Accuracy: 1.0\n",
      "batch: 600 Accuracy: 1.0\n",
      "batch: 601 Accuracy: 1.0\n",
      "batch: 602 Accuracy: 1.0\n",
      "batch: 603 Accuracy: 1.0\n",
      "batch: 604 Accuracy: 1.0\n",
      "batch: 605 Accuracy: 1.0\n",
      "batch: 606 Accuracy: 1.0\n",
      "batch: 607 Accuracy: 1.0\n",
      "batch: 608 Accuracy: 1.0\n",
      "batch: 609 Accuracy: 1.0\n",
      "batch: 610 Accuracy: 1.0\n",
      "batch: 611 Accuracy: 1.0\n",
      "batch: 612 Accuracy: 1.0\n",
      "batch: 613 Accuracy: 1.0\n",
      "batch: 614 Accuracy: 1.0\n",
      "batch: 615 Accuracy: 1.0\n",
      "batch: 616 Accuracy: 1.0\n",
      "batch: 617 Accuracy: 1.0\n",
      "batch: 618 Accuracy: 1.0\n",
      "batch: 619 Accuracy: 1.0\n",
      "batch: 620 Accuracy: 0.875\n",
      "batch: 621 Accuracy: 1.0\n",
      "batch: 622 Accuracy: 1.0\n",
      "batch: 623 Accuracy: 1.0\n",
      "batch: 624 Accuracy: 1.0\n",
      "batch: 625 Accuracy: 1.0\n",
      "batch: 626 Accuracy: 1.0\n",
      "batch: 627 Accuracy: 1.0\n",
      "batch: 628 Accuracy: 1.0\n",
      "batch: 629 Accuracy: 1.0\n",
      "batch: 630 Accuracy: 1.0\n",
      "batch: 631 Accuracy: 1.0\n",
      "batch: 632 Accuracy: 1.0\n",
      "batch: 633 Accuracy: 1.0\n",
      "batch: 634 Accuracy: 1.0\n",
      "batch: 635 Accuracy: 1.0\n",
      "batch: 636 Accuracy: 1.0\n",
      "batch: 637 Accuracy: 1.0\n",
      "batch: 638 Accuracy: 1.0\n",
      "batch: 639 Accuracy: 1.0\n",
      "batch: 640 Accuracy: 1.0\n",
      "batch: 641 Accuracy: 1.0\n",
      "batch: 642 Accuracy: 1.0\n",
      "batch: 643 Accuracy: 1.0\n",
      "batch: 644 Accuracy: 1.0\n",
      "batch: 645 Accuracy: 1.0\n",
      "batch: 646 Accuracy: 1.0\n",
      "batch: 647 Accuracy: 1.0\n",
      "batch: 648 Accuracy: 1.0\n",
      "batch: 649 Accuracy: 1.0\n",
      "batch: 650 Accuracy: 1.0\n",
      "batch: 651 Accuracy: 1.0\n",
      "batch: 652 Accuracy: 1.0\n",
      "batch: 653 Accuracy: 1.0\n",
      "batch: 654 Accuracy: 1.0\n",
      "batch: 655 Accuracy: 1.0\n",
      "batch: 656 Accuracy: 1.0\n",
      "batch: 657 Accuracy: 1.0\n",
      "batch: 658 Accuracy: 1.0\n",
      "batch: 659 Accuracy: 1.0\n",
      "batch: 660 Accuracy: 1.0\n",
      "batch: 661 Accuracy: 1.0\n",
      "batch: 662 Accuracy: 1.0\n",
      "batch: 663 Accuracy: 1.0\n",
      "batch: 664 Accuracy: 1.0\n",
      "batch: 665 Accuracy: 1.0\n",
      "batch: 666 Accuracy: 1.0\n",
      "batch: 667 Accuracy: 1.0\n",
      "batch: 668 Accuracy: 1.0\n",
      "batch: 669 Accuracy: 1.0\n",
      "batch: 670 Accuracy: 1.0\n",
      "batch: 671 Accuracy: 1.0\n",
      "batch: 672 Accuracy: 1.0\n",
      "batch: 673 Accuracy: 1.0\n",
      "batch: 674 Accuracy: 1.0\n",
      "batch: 675 Accuracy: 1.0\n",
      "batch: 676 Accuracy: 1.0\n",
      "batch: 677 Accuracy: 1.0\n",
      "batch: 678 Accuracy: 1.0\n",
      "batch: 679 Accuracy: 1.0\n",
      "batch: 680 Accuracy: 1.0\n",
      "batch: 681 Accuracy: 1.0\n",
      "batch: 682 Accuracy: 1.0\n",
      "batch: 683 Accuracy: 1.0\n",
      "batch: 684 Accuracy: 1.0\n",
      "batch: 685 Accuracy: 1.0\n",
      "batch: 686 Accuracy: 1.0\n",
      "batch: 687 Accuracy: 1.0\n",
      "batch: 688 Accuracy: 1.0\n",
      "batch: 689 Accuracy: 1.0\n",
      "batch: 690 Accuracy: 1.0\n",
      "batch: 691 Accuracy: 1.0\n",
      "batch: 692 Accuracy: 1.0\n",
      "batch: 693 Accuracy: 1.0\n",
      "batch: 694 Accuracy: 1.0\n",
      "batch: 695 Accuracy: 1.0\n",
      "batch: 696 Accuracy: 1.0\n",
      "batch: 697 Accuracy: 1.0\n",
      "batch: 698 Accuracy: 1.0\n",
      "batch: 699 Accuracy: 1.0\n",
      "batch: 700 Accuracy: 1.0\n",
      "batch: 701 Accuracy: 1.0\n",
      "batch: 702 Accuracy: 1.0\n",
      "batch: 703 Accuracy: 1.0\n",
      "batch: 704 Accuracy: 1.0\n",
      "batch: 705 Accuracy: 1.0\n",
      "batch: 706 Accuracy: 1.0\n",
      "batch: 707 Accuracy: 1.0\n",
      "batch: 708 Accuracy: 1.0\n",
      "batch: 709 Accuracy: 1.0\n",
      "batch: 710 Accuracy: 1.0\n",
      "batch: 711 Accuracy: 1.0\n",
      "batch: 712 Accuracy: 1.0\n",
      "batch: 713 Accuracy: 1.0\n",
      "batch: 714 Accuracy: 1.0\n",
      "batch: 715 Accuracy: 1.0\n",
      "batch: 716 Accuracy: 1.0\n",
      "batch: 717 Accuracy: 1.0\n",
      "batch: 718 Accuracy: 1.0\n",
      "batch: 719 Accuracy: 1.0\n",
      "batch: 720 Accuracy: 1.0\n",
      "batch: 721 Accuracy: 1.0\n",
      "batch: 722 Accuracy: 1.0\n",
      "batch: 723 Accuracy: 1.0\n",
      "batch: 724 Accuracy: 1.0\n",
      "batch: 725 Accuracy: 1.0\n",
      "batch: 726 Accuracy: 1.0\n",
      "batch: 727 Accuracy: 1.0\n",
      "batch: 728 Accuracy: 1.0\n",
      "batch: 729 Accuracy: 1.0\n",
      "batch: 730 Accuracy: 1.0\n",
      "batch: 731 Accuracy: 1.0\n",
      "batch: 732 Accuracy: 1.0\n",
      "batch: 733 Accuracy: 1.0\n",
      "batch: 734 Accuracy: 1.0\n",
      "batch: 735 Accuracy: 1.0\n",
      "batch: 736 Accuracy: 1.0\n",
      "batch: 737 Accuracy: 1.0\n",
      "batch: 738 Accuracy: 1.0\n",
      "batch: 739 Accuracy: 1.0\n",
      "batch: 740 Accuracy: 1.0\n",
      "batch: 741 Accuracy: 1.0\n",
      "batch: 742 Accuracy: 1.0\n",
      "batch: 743 Accuracy: 1.0\n",
      "batch: 744 Accuracy: 1.0\n",
      "batch: 745 Accuracy: 1.0\n",
      "batch: 746 Accuracy: 1.0\n",
      "batch: 747 Accuracy: 1.0\n",
      "batch: 748 Accuracy: 1.0\n",
      "batch: 749 Accuracy: 1.0\n",
      "batch: 750 Accuracy: 1.0\n",
      "batch: 751 Accuracy: 1.0\n",
      "batch: 752 Accuracy: 1.0\n",
      "batch: 753 Accuracy: 1.0\n",
      "batch: 754 Accuracy: 1.0\n",
      "batch: 755 Accuracy: 1.0\n",
      "batch: 756 Accuracy: 1.0\n",
      "batch: 757 Accuracy: 1.0\n",
      "batch: 758 Accuracy: 1.0\n",
      "batch: 759 Accuracy: 1.0\n",
      "batch: 760 Accuracy: 1.0\n",
      "batch: 761 Accuracy: 1.0\n",
      "batch: 762 Accuracy: 1.0\n",
      "batch: 763 Accuracy: 1.0\n",
      "batch: 764 Accuracy: 1.0\n",
      "batch: 765 Accuracy: 1.0\n",
      "batch: 766 Accuracy: 1.0\n",
      "batch: 767 Accuracy: 1.0\n",
      "batch: 768 Accuracy: 1.0\n",
      "batch: 769 Accuracy: 1.0\n",
      "batch: 770 Accuracy: 1.0\n",
      "batch: 771 Accuracy: 1.0\n",
      "batch: 772 Accuracy: 1.0\n",
      "batch: 773 Accuracy: 1.0\n",
      "batch: 774 Accuracy: 1.0\n",
      "batch: 775 Accuracy: 1.0\n",
      "batch: 776 Accuracy: 1.0\n",
      "batch: 777 Accuracy: 0.875\n",
      "batch: 778 Accuracy: 1.0\n",
      "batch: 779 Accuracy: 1.0\n",
      "batch: 780 Accuracy: 1.0\n",
      "batch: 781 Accuracy: 1.0\n",
      "batch: 782 Accuracy: 1.0\n",
      "batch: 783 Accuracy: 1.0\n",
      "batch: 784 Accuracy: 1.0\n",
      "batch: 785 Accuracy: 1.0\n",
      "batch: 786 Accuracy: 1.0\n",
      "batch: 787 Accuracy: 1.0\n",
      "batch: 788 Accuracy: 1.0\n",
      "batch: 789 Accuracy: 1.0\n",
      "batch: 790 Accuracy: 1.0\n",
      "batch: 791 Accuracy: 1.0\n",
      "batch: 792 Accuracy: 1.0\n",
      "batch: 793 Accuracy: 1.0\n",
      "batch: 794 Accuracy: 1.0\n",
      "overall accuracy: 0.9990554156171285\n"
     ]
    }
   ],
   "source": [
    "test(model, dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'deep-learning-flower-identifier' already exists and is not an empty directory.\n",
      "Requirement already satisfied: airtable in /data/anaconda/envs/py35/lib/python3.5/site-packages (0.3.1)\n",
      "Requirement already satisfied: requests>=2.5.3 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from airtable) (2.18.4)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from requests>=2.5.3->airtable) (1.22)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from requests>=2.5.3->airtable) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from requests>=2.5.3->airtable) (2.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from requests>=2.5.3->airtable) (2018.8.24)\n",
      "5.3.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 0.875\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 0.75\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 0.875\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 0.875\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 0.875\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Batch accuracy (Size 8): 1.0\n",
      "Mean accuracy: 0.9940000176429749\n"
     ]
    }
   ],
   "source": [
    "google_test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
